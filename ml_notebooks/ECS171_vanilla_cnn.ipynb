{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e31610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch Imports\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95736d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader Class\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "import torchvision.io\n",
    "from torchvision.io.image import read_image\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2 as io\n",
    "\n",
    "class Ham10000(Dataset):\n",
    "    def __init__(self, csv_file, directory, transform, datasetname):\n",
    "        self.annotations = pd.read_csv(f\"{csv_file}\")\n",
    "        self.img_root_dir = 'dataverse_files'\n",
    "        self.transform = transform\n",
    "        self.datasetname = datasetname\n",
    "        self.csv_file = csv_file\n",
    "        self.directory = directory\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = ''\n",
    "        img_path = os.path.join(self.directory, self.datasetname, self.annotations.iloc[idx, 1])\n",
    "        img_path += \".jpg\"\n",
    "\n",
    "        x_img = io.imread(img_path)\n",
    "        if self.transform:\n",
    "            x_img = self.transform(x_img)\n",
    "\n",
    "        y_label = torch.tensor(int(self.annotations.iloc[idx, 7]))\n",
    "\n",
    "        return (x_img, y_label)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a6cd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KFold Function\n",
    "from sklearn.model_selection import KFold\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "\n",
    "# pass in model constructor\n",
    "def kfold(model, dataset, device, num_folds=5, num_epochs=10, loss_function=nn.CrossEntropyLoss()):\n",
    "\n",
    "  results = {}\n",
    "  kfold = KFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "  print('--------------------------------')\n",
    "  # K-fold Cross Validation model evaluation\n",
    "  for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n",
    "\n",
    "    # Print\n",
    "    print(f'FOLD {fold}')\n",
    "    print('--------------------------------')\n",
    "    \n",
    "    # Sample elements randomly from a given list of ids, no replacement.\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
    "    \n",
    "    # Define data loaders for training and testing data in this fold\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "                      dataset, \n",
    "                      batch_size=32, sampler=train_subsampler, num_workers=4)\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "                      dataset,\n",
    "                      batch_size=32, sampler=test_subsampler, num_workers=4)\n",
    "    \n",
    "    # Init the neural network\n",
    "    network = copy.deepcopy(model)\n",
    "    network.to(device)\n",
    "    \n",
    "    # Initialize optimizer\n",
    "    optimizer = torch.optim.Adam(network.parameters(), lr=0.001)\n",
    "    total_train_loss = []\n",
    "    fold_train_acc = []\n",
    "    fold_test_acc = []\n",
    "    for epoch in range(0, num_epochs):\n",
    "      print(f'Starting epoch {epoch+1}', '-', num_epochs)\n",
    "      correct, total = 0, 0\n",
    "      current_loss = 0.0\n",
    "      network.train()\n",
    "      train_loss = 0\n",
    "      for i, data in enumerate(trainloader, 0):\n",
    "        inputs, targets = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = network(inputs)\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        loss = loss_function(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "        \n",
    "        # Print statistics\n",
    "        train_loss+= loss.item()\n",
    "        current_loss += loss.item()\n",
    "      fold_train_acc.append(100.0 * correct / total)\n",
    "\n",
    "      network.eval()\n",
    "    # Evaluation for this fold\n",
    "      correct, total = 0, 0\n",
    "      total_labels, total_preds = [],[]\n",
    "      with torch.no_grad():\n",
    "        for i, data in enumerate(testloader, 0):\n",
    "          inputs, targets = data[0].to(device), data[1].to(device)\n",
    "          outputs = network(inputs)\n",
    "          _, predicted = torch.max(outputs.data, 1)\n",
    "          total_labels.extend(targets.data.cpu().detach().numpy())\n",
    "          total_preds.extend(predicted.cpu().detach().numpy())\n",
    "          total += targets.size(0)\n",
    "          correct += (predicted == targets).sum().item()\n",
    "   \n",
    "        results[fold] = 100.0 * (correct / total)\n",
    "\n",
    "      total_train_loss.append(train_loss)\n",
    "      fold_test_acc.append(100.0 * correct / total)\n",
    "    torch.save(network, \"./vanilla_cnn_aug_{0}.pth\".format(fold))\n",
    "\n",
    "    print(\"fold_train_acc: \", fold_train_acc)\n",
    "    print(\"fold_test_acc: \", fold_test_acc)\n",
    "    print(\"total_train_loss: \", total_train_loss)\n",
    "  print(f'K-FOLD CROSS VALIDATION RESULTS FOR {num_folds} FOLDS')\n",
    "  print('--------------------------------')\n",
    "  sum = 0.0\n",
    "  for key, value in results.items():\n",
    "    print(f'Fold {key}: {value} %')\n",
    "    sum += value\n",
    "  print(f'Average: {sum/len(results.items())} %')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54ead25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Class Defintion\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 16, 3),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(16, 32, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, 3),\n",
    "            nn.MaxPool2d(2, 2),\n",
    " \n",
    "            nn.Conv2d(32, 64, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 3),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 52 * 71, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 7)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "      \n",
    "    def save(self,path):\n",
    "      torch.save(self.network.state_dict(), path)\n",
    "    \n",
    "    def load(self, path):\n",
    "      self.network.load_state_dict(torch.load(path))\n",
    "\n",
    "    def train_net(self,epochs, lr, train_loader, test_loader,opt_func, loss_func):\n",
    "      print(self.device)\n",
    "      self.network.train()\n",
    "\n",
    "      optimizer = opt_func(self.network.parameters(), lr=lr)\n",
    "\n",
    "      for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "          inputs, labels = data\n",
    "          inputs, labels = inputs.to(self.device),  labels.to(self.device) # move to GPU is possible\n",
    "          \n",
    "          # zero the parameter gradients\n",
    "          optimizer.zero_grad()\n",
    "\n",
    "          # forward + backward + optimize\n",
    "          outputs = self.network(inputs)\n",
    "          loss = loss_func(outputs, labels)\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "\n",
    "          # print statistics\n",
    "          running_loss += loss.item()\n",
    "          if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "              print(f'[{epoch + 1}, {i + 1:5d}] train loss: {running_loss / 2000:.3f}')\n",
    "              running_loss = 0.0\n",
    "\n",
    "      print('Finished Training')\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def test(self,test_loader):\n",
    "      self.network.eval()\n",
    "      correct = 0\n",
    "      total = 0\n",
    "      for data in test_loader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(self.device),  labels.to(self.device) \n",
    "\n",
    "        predictions = self.network(inputs)\n",
    "        _, predicted = torch.max(predictions.data, 1)\n",
    "\n",
    "        total += len(data)\n",
    "        correct += torch.sum(predicted == labels).item()\n",
    "      \n",
    "      return correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546d18c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run K-Folds (Vanilla CNN w/o Augmentations)\n",
    "csv_file = \"./dataverse_files/HAM10000_metadata.csv\"\n",
    "directory = \"./dataverse_files\"\n",
    "\n",
    "transform = transforms.Compose(\n",
    "      [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1411, 0.0923, 0.5270), (0.3407, 0.3058, 0.2824))\n",
    "      ]\n",
    "    )\n",
    "\n",
    "batch_size = 32\n",
    "datasetname = \"HAM10000_images_off\"\n",
    "dataset = Ham10000(csv_file, directory, transform, datasetname)\n",
    "\n",
    "model = Net()\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "kfold(model, dataset, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6038afb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Results\n",
    "\n",
    "fold1trainacc = [66.13829256115827, 66.74987518721917, 67.38642036944583, 67.44882675986021, 67.37393909136296, 66.88716924613081, 66.97453819271094, 66.60009985022467, 66.91213180229656, 66.91213180229656]\n",
    "fold1testacc = [66.00099850224663, 68.09785322016974, 67.3989016475287, 68.04792810783825, 67.04942586120819, 67.2491263105342, 66.99950074887668, 67.09935097353969, 67.09935097353969, 67.09935097353969]\n",
    "total1_train_loss =  [538.7118993103504, 483.97415643930435, 451.3105698078871, 438.3517017364502, 480.92257146537304, 469.55596220493317, 494.6970077455044, 885.2272503376007, 602.0155299901962, 577.3336101174355]\n",
    "\n",
    "fold2trainacc = [66.0134797803295, 66.2755866200699, 66.71243135297054, 66.72491263105341, 66.72491263105341, 66.72491263105341, 66.72491263105341, 66.72491263105341, 66.72491263105341, 66.72491263105341]\n",
    "fold2testacc = [67.84822765851223, 67.84822765851223, 67.84822765851223, 67.84822765851223, 67.84822765851223, 67.84822765851223, 67.84822765851223, 67.84822765851223, 67.84822765851223, 67.84822765851223]\n",
    "total2_train_loss=  [535.1830442249775, 519.0360944867134, 579.314981341362, 577.0459206104279, 577.2824673652649, 574.9433191120625, 574.8410721719265, 574.2682507038116, 573.5881962776184, 574.595221221447]\n",
    "\n",
    "fold3trainacc = [66.2256615077384, 67.41138292561158, 67.22416375436845, 67.51123315027459, 67.32401397903145, 67.1492760858712, 67.16175736395407, 67.41138292561158, 67.83574638042936, 68.24762855716425]\n",
    "fold3testacc = [65.15227159261109, 65.5017473789316, 65.35197204193709, 65.3020469296056, 65.35197204193709, 65.35197204193709, 65.3020469296056, 65.35197204193709, 66.99950074887668, 67.2491263105342]\n",
    "total3_train_loss =  [525.2064574062824, 459.3988243639469, 469.3730876892805, 450.1371608674526, 470.49838972091675, 500.6157111674547, 466.9559631049633, 444.4675491452217, 436.5911014974117, 425.83565333485603]\n",
    "\n",
    "fold4trainacc = [65.80129805292061, 67.44882675986021, 67.83574638042936, 68.26010983524714, 67.68597104343485, 68.02296555167248, 68.47229156265601, 69.9700449326011, 71.80479281078382, 73.52720918622067]\n",
    "fold4testacc = [66.94957563654518, 68.69695456814777, 68.59710434348477, 69.0464303544683, 69.34598102845732, 68.19770344483275, 69.99500748876684, 69.29605591612581, 69.24613080379432, 69.0464303544683]\n",
    "total4_train_loss=  [517.8645753860474, 467.08774450421333, 448.84585615992546, 435.18410378694534, 441.71014964580536, 437.12786234915257, 426.4030050933361, 400.3480721116066, 370.62979401648045, 369.1445248275995]\n",
    "\n",
    "fold5trainacc = [65.95107338991512, 66.74987518721917, 66.58761857214179, 65.05242136794807, 66.71243135297054, 66.7873190214678, 66.7873190214678, 66.7873190214678, 66.7873190214678, 66.7873190214678]\n",
    "fold5testacc = [68.74687968047928, 67.59860209685472, 67.64852720918623, 67.59860209685472, 67.59860209685472, 67.59860209685472, 67.59860209685472, 67.59860209685472, 67.59860209685472, 67.59860209685472]\n",
    "total5_train_loss=  [517.5363972485065, 507.1936755105853, 498.6692821085453, 573.3577049970627, 702.2055148482323, 604.909136891365, 581.6331693530083, 574.5571982860565, 572.0612896680832, 571.0651732683182]\n",
    "\n",
    "\n",
    "total_train_loss_append = [total1_train_loss, total2_train_loss, total3_train_loss, total4_train_loss, total5_train_loss]\n",
    "total_train_loss = []\n",
    "for i in range(len(total1_train_loss)):\n",
    "  total_loss = []\n",
    "  for j in range(len(total_train_loss_append)):\n",
    "    total_loss.append(total_train_loss_append[j][i])\n",
    "  total_train_loss.append(sum(total_loss) / len(total_loss))\n",
    "print(total_train_loss)\n",
    "\n",
    "\n",
    "total_train_acc_append = [fold1trainacc, fold2trainacc, fold3trainacc, fold4trainacc, fold5trainacc]\n",
    "\n",
    "total_train_acc = []\n",
    "for i in range(len(total1_train_loss)):\n",
    "  total_acc = []\n",
    "  for j in range(len(total_train_acc_append)):\n",
    "    total_acc.append(total_train_acc_append[j][i])\n",
    "  total_train_acc.append(sum(total_acc) / len(total_acc))\n",
    "print(total_train_acc)\n",
    "\n",
    "\n",
    "total_test_acc_append = [fold1testacc, fold2testacc, fold3testacc, fold4testacc, fold5testacc]\n",
    "\n",
    "total_test_acc = []\n",
    "for i in range(len(total1_train_loss)):\n",
    "  total_acc = []\n",
    "  for j in range(len(total_test_acc_append)):\n",
    "    total_acc.append(total_test_acc_append[j][i])\n",
    "  total_test_acc.append(sum(total_acc) / len(total_acc))\n",
    "print(total_test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174dffc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Graph\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "epochs = list(range(0,len(total_train_loss)))\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'Epoch': epochs,\n",
    "    'Loss': total_train_loss,\n",
    "})\n",
    "\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "sns.lineplot(x='Epoch',y='Loss',data=data).set(title=\"Train Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3942dd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy Graph\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "epochs = list(range(0,len(total_train_acc)))\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'Epoch': epochs,\n",
    "    'Training Accuracy': total_train_acc,\n",
    "    'Testing Accuracy': total_test_acc\n",
    "})\n",
    "\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "sns.lineplot(x='Epoch',y='Accuracy',hue='variable',data=pd.melt(data, ['Epoch'],value_name=\"Accuracy\")).set(title=\"Train vs Test Accuracy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45596190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanilla CNN w/Updated Layer Sizes (for Crop Augmentation)\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 16, 3),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(16, 32, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, 3),\n",
    "            nn.MaxPool2d(2, 2),\n",
    " \n",
    "            nn.Conv2d(32, 64, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 3),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 24 * 24, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 7)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "      \n",
    "    def save(self,path):\n",
    "      torch.save(self.network.state_dict(), path)\n",
    "    \n",
    "    def load(self, path):\n",
    "      self.network.load_state_dict(torch.load(path))\n",
    "\n",
    "    def train_net(self,epochs, lr, train_loader, test_loader,opt_func, loss_func):\n",
    "      print(self.device)\n",
    "      self.network.train()\n",
    "\n",
    "      optimizer = opt_func(self.network.parameters(), lr=lr)\n",
    "\n",
    "      for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "          inputs, labels = data\n",
    "          inputs, labels = inputs.to(self.device),  labels.to(self.device) # move to GPU is possible\n",
    "          \n",
    "          # zero the parameter gradients\n",
    "          optimizer.zero_grad()\n",
    "\n",
    "          # forward + backward + optimize\n",
    "          outputs = self.network(inputs)\n",
    "          loss = loss_func(outputs, labels)\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "\n",
    "          # print statistics\n",
    "          running_loss += loss.item()\n",
    "          if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "              print(f'[{epoch + 1}, {i + 1:5d}] train loss: {running_loss / 2000:.3f}')\n",
    "              running_loss = 0.0\n",
    "\n",
    "      print('Finished Training')\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def test(self,test_loader):\n",
    "      self.network.eval()\n",
    "      correct = 0\n",
    "      total = 0\n",
    "      for data in test_loader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(self.device),  labels.to(self.device) \n",
    "\n",
    "        predictions = self.network(inputs)\n",
    "        _, predicted = torch.max(predictions.data, 1)\n",
    "\n",
    "        total += len(data)\n",
    "        correct += torch.sum(predicted == labels).item()\n",
    "      \n",
    "      return correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c19e265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run K-Folds (Vanilla CNN w/Augmentations)\n",
    "csv_file = \"./dataverse_files/HAM10000_metadata.csv\"\n",
    "directory = \"./dataverse_files\"\n",
    "\n",
    "transform = transforms.Compose(\n",
    "      [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.RandomRotation(degrees=(13)),\n",
    "        transforms.ColorJitter(brightness=.5, hue=.3),\n",
    "        transforms.Normalize((0.1411, 0.0923, 0.5270), (0.3407, 0.3058, 0.2824))\n",
    "          \n",
    "      ]\n",
    "    )\n",
    "\n",
    "batch_size = 32\n",
    "datasetname = \"HAM10000_images_off\"\n",
    "dataset = Ham10000(csv_file, directory, transform, datasetname)\n",
    "\n",
    "model = Net()\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "kfold(model, dataset, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa06aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Results\n",
    "\n",
    "fold1trainacc = [66.67498751872192, 66.96205691462806, 66.96205691462806, 66.89965052421368, 66.96205691462806, 66.84972541188218, 66.96205691462806, 66.9370943584623, 66.96205691462806, 66.96205691462806]\n",
    "fold1testacc = [66.89965052421368, 66.89965052421368, 66.89965052421368, 66.89965052421368, 66.89965052421368, 66.89965052421368, 66.89965052421368, 66.89965052421368, 66.89965052421368, 66.89965052421368]\n",
    "total1_train_loss =  [587.4317564666271, 555.7362589538097, 533.5898955762386, 531.2370494306087, 526.737876355648, 535.5994701683521, 526.8527587652206, 522.9258488416672, 517.0247338712215, 515.8226729631424]\n",
    "\n",
    "fold2trainacc = [66.8372441337993, 67.01198202695956, 67.01198202695956, 67.01198202695956, 67.01198202695956, 66.40039940089865, 67.01198202695956, 67.01198202695956, 67.01198202695956, 67.01198202695956]\n",
    "fold2testacc = [66.69995007488767, 66.69995007488767, 66.69995007488767, 66.69995007488767, 66.69995007488767, 66.69995007488767, 66.69995007488767, 66.69995007488767, 66.69995007488767, 66.69995007488767]\n",
    "total2_train_loss= [577.734813272953, 572.6809532642365, 571.536487609148, 569.8435501754284, 571.6515056490898, 618.4085868299007, 571.6554693877697, 570.0561816990376, 569.7348641455173, 568.4866904914379]\n",
    "\n",
    "fold3trainacc = [66.60009985022467, 66.88716924613081, 66.74987518721917, 66.7373939091363, 66.81228157763356, 66.89965052421368, 66.89965052421368, 66.89965052421368, 66.89965052421368, 66.89965052421368]\n",
    "fold3testacc = [67.1492760858712, 67.1492760858712, 67.1492760858712, 67.1492760858712, 67.1492760858712, 67.1492760858712, 67.1492760858712, 67.1492760858712, 67.1492760858712, 67.1492760858712]\n",
    "total3_train_loss = [569.2010554969311, 539.6914494037628, 549.1378348469734, 544.0546608269215, 585.012408643961, 570.9453338682652, 570.7000705003738, 569.9297728538513, 569.4372866749763, 569.206324338913]\n",
    "\n",
    "fold4trainacc = [66.41288067898152, 66.56265601597603, 66.56265601597603, 66.45032451323016, 66.51273090364454, 66.5751372940589, 66.56265601597603, 66.58761857214179, 66.5751372940589, 66.55017473789316]\n",
    "fold4testacc = [68.44732900649026, 68.44732900649026, 68.44732900649026, 68.44732900649026, 68.44732900649026, 68.44732900649026, 68.44732900649026, 68.44732900649026, 68.44732900649026, 68.44732900649026]\n",
    "total4_train_loss= [567.0311739295721, 541.0548948049545, 538.174273788929, 531.5657752454281, 546.1777322292328, 521.7583494782448, 519.0866031050682, 523.7675650566816, 510.0987524539232, 513.779454022646]\n",
    "\n",
    "fold5trainacc = [67.09935097353969, 67.2990514228657, 67.13679480778832, 67.26160758861707, 67.2990514228657, 67.16175736395407, 67.2990514228657, 67.2990514228657, 67.23664503245132, 67.27408886669996]\n",
    "fold5testacc = [65.5516724912631, 65.5516724912631, 65.5516724912631, 65.5516724912631, 65.5516724912631, 65.5516724912631, 65.5516724912631, 65.5516724912631, 65.5516724912631, 65.5516724912631]\n",
    "total5_train_loss=  [572.5991007685661, 534.6310192644596, 543.1988770961761, 553.1111596822739, 519.6321686804295, 567.483310252428, 532.9011108875275, 522.2456321418285, 539.5499770641327, 567.5038605332375]\n",
    "\n",
    "\n",
    "total_train_loss_append = [total1_train_loss, total2_train_loss, total3_train_loss, total4_train_loss, total5_train_loss]\n",
    "total_train_loss = []\n",
    "for i in range(len(total1_train_loss)):\n",
    "  total_loss = []\n",
    "  for j in range(len(total_train_loss_append)):\n",
    "    total_loss.append(total_train_loss_append[j][i])\n",
    "  total_train_loss.append(sum(total_loss) / len(total_loss))\n",
    "print(total_train_loss)\n",
    "\n",
    "\n",
    "total_train_acc_append = [fold1trainacc, fold2trainacc, fold3trainacc, fold4trainacc, fold5trainacc]\n",
    "\n",
    "total_train_acc = []\n",
    "for i in range(len(total1_train_loss)):\n",
    "  total_acc = []\n",
    "  for j in range(len(total_train_acc_append)):\n",
    "    total_acc.append(total_train_acc_append[j][i])\n",
    "  total_train_acc.append(sum(total_acc) / len(total_acc))\n",
    "print(total_train_acc)\n",
    "\n",
    "\n",
    "total_test_acc_append = [fold1testacc, fold2testacc, fold3testacc, fold4testacc, fold5testacc]\n",
    "\n",
    "total_test_acc = []\n",
    "for i in range(len(total1_train_loss)):\n",
    "  total_acc = []\n",
    "  for j in range(len(total_test_acc_append)):\n",
    "    total_acc.append(total_test_acc_append[j][i])\n",
    "  total_test_acc.append(sum(total_acc) / len(total_acc))\n",
    "print(total_test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d09bee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Graph\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "epochs = list(range(0,len(total_train_loss)))\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'Epoch': epochs,\n",
    "    'Loss': total_train_loss,\n",
    "})\n",
    "\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "sns.lineplot(x='Epoch',y='Loss',data=data).set(title=\"Train Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5746d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy Graph\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "epochs = list(range(0,len(total_train_acc)))\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'Epoch': epochs,\n",
    "    'Training Accuracy': total_train_acc,\n",
    "    'Testing Accuracy': total_test_acc\n",
    "})\n",
    "\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "sns.lineplot(x='Epoch',y='Accuracy',hue='variable',data=pd.melt(data, ['Epoch'],value_name=\"Accuracy\")).set(title=\"Train vs Test Accuracy\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
